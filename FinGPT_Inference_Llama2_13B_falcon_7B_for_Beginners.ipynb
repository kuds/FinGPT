{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuds/FinGPT/blob/master/FinGPT_Inference_Llama2_13B_falcon_7B_for_Beginners.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTV-l4nMGmtU"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install peft\n",
        "!pip install sentencepiece\n",
        "!pip install accelerate\n",
        "!pip install torch\n",
        "!pip install peft\n",
        "!pip install datasets\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for the `load_in_8bit=True` error\n",
        "!pip install protobuf transformers cpm_kernels torch gradio mdtex2html sentencepiece accelerate"
      ],
      "metadata": {
        "id": "Mtui5C82IwR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference with Single Task: Sentiment"
      ],
      "metadata": {
        "id": "uTyL2Ct70bmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM, LlamaTokenizerFast, BitsAndBytesConfig\n",
        "from peft import PeftModel"
      ],
      "metadata": {
        "id": "UFzsvG4ZbXVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Models\n",
        "base_model = \"NousResearch/Llama-2-13b-hf\"\n",
        "peft_model = \"FinGPT/fingpt-sentiment_llama2-13b_lora\"\n",
        "tokenizer = LlamaTokenizerFast.from_pretrained(base_model, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "model = LlamaForCausalLM.from_pretrained(base_model,\n",
        "                                         trust_remote_code=True,\n",
        "                                         device_map = \"cuda:0\",\n",
        "                                         quantization_config=quantization_config)\n",
        "model = PeftModel.from_pretrained(model, peft_model)\n",
        "model = model.eval()"
      ],
      "metadata": {
        "id": "ZMxTm7wFYrla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make prompts\n",
        "prompt = [\n",
        "'''Instruction: What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}\n",
        "Input: FINANCING OF ASPOCOMP 'S GROWTH Aspocomp is aggressively pursuing its growth strategy by increasingly focusing on technologically more demanding HDI printed circuit boards PCBs .\n",
        "Answer: ''',\n",
        "'''Instruction: What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}\n",
        "Input: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n",
        "Answer: ''',\n",
        "'''Instruction: What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}\n",
        "Input: A tinyurl link takes users to a scamming site promising that users can earn thousands of dollars by becoming a Google ( NASDAQ : GOOG ) Cash advertiser .\n",
        "Answer: ''',\n",
        "]\n",
        "\n",
        "# Generate results\n",
        "tokens = tokenizer(prompt, return_tensors='pt', padding=True, max_length=512)\n",
        "tokens = {key: value.to(model.device) for key, value in tokens.items()} # Move tensors to the same device as the model\n",
        "res = model.generate(**tokens, max_length=512)\n",
        "res_sentences = [tokenizer.decode(i) for i in res]\n",
        "out_text = [o.split(\"Answer: \")[1] for o in res_sentences]\n",
        "\n",
        "# show results\n",
        "for sentiment in out_text:\n",
        "    print(sentiment)\n",
        "\n",
        "# Output:\n",
        "# positive\n",
        "# neutral\n",
        "# negative"
      ],
      "metadata": {
        "id": "0tTwD4E9Yroi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference with Multi-Task\n",
        "* Please Restart runtime, the memory is not enough to run two tasks"
      ],
      "metadata": {
        "id": "07-KagX6YtM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM, LlamaTokenizerFast\n",
        "from peft import PeftModel"
      ],
      "metadata": {
        "id": "QJN4tCASbZxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datasets\n",
        "\n",
        "\n",
        "template_dict = {\n",
        "    'default': 'Instruction: {instruction}\\nInput: {input}\\nAnswer: '\n",
        "}\n",
        "\n",
        "lora_module_dict = {\n",
        "    'chatglm2': ['query_key_value'],\n",
        "    'falcon': ['query_key_value'],\n",
        "    'bloom': ['query_key_value'],\n",
        "    'internlm': ['q_proj', 'k_proj', 'v_proj'],\n",
        "    'llama2': ['q_proj', 'k_proj', 'v_proj'],\n",
        "    'qwen': [\"c_attn\"],\n",
        "    'mpt': ['Wqkv'],\n",
        "}\n",
        "\n",
        "\n",
        "def get_prompt(template, instruction, input):\n",
        "\n",
        "    if instruction:\n",
        "        return template_dict[template].format(instruction=instruction, input=input)\n",
        "    else:\n",
        "        return input\n",
        "\n",
        "\n",
        "def test_mapping(args, feature):\n",
        "\n",
        "    prompt = get_prompt(\n",
        "        args.instruct_template,\n",
        "        feature['instruction'],\n",
        "        feature['input']\n",
        "    )\n",
        "    return {\n",
        "        \"prompt\": prompt,\n",
        "    }\n",
        "\n",
        "\n",
        "def tokenize(args, tokenizer, feature):\n",
        "\n",
        "    prompt = get_prompt(\n",
        "        args.instruct_template,\n",
        "        feature['instruction'],\n",
        "        feature['input']\n",
        "    )\n",
        "    prompt_ids = tokenizer(\n",
        "        prompt, padding=False,\n",
        "        max_length=args.max_length, truncation=True\n",
        "    )['input_ids']\n",
        "    target_ids = tokenizer(\n",
        "        feature['output'].strip(), padding=False,\n",
        "        max_length=args.max_length, truncation=True,\n",
        "        add_special_tokens=False\n",
        "    )['input_ids']\n",
        "\n",
        "    input_ids = prompt_ids + target_ids\n",
        "    exceed_max_length = len(input_ids) >= args.max_length\n",
        "\n",
        "    # Add EOS Token\n",
        "    if input_ids[-1] != tokenizer.eos_token_id and not exceed_max_length:\n",
        "        input_ids.append(tokenizer.eos_token_id)\n",
        "\n",
        "    label_ids = [tokenizer.pad_token_id] * len(prompt_ids) + input_ids[len(prompt_ids):]\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"labels\": label_ids,\n",
        "        \"exceed_max_length\": exceed_max_length\n",
        "    }\n",
        "\n",
        "\n",
        "def parse_model_name(name, from_remote=False):\n",
        "\n",
        "    if name == 'chatglm2':\n",
        "        return 'THUDM/chatglm2-6b' if from_remote else 'base_models/chatglm2-6b'\n",
        "    elif name == 'llama2':\n",
        "        return 'meta-llama/Llama-2-7b-hf' if from_remote else 'base_models/Llama-2-7b-hf'\n",
        "        # return 'NousResearch/Llama-2-7b-hf' if from_remote else 'base_models/Llama-2-7b-hf-nous'\n",
        "    elif name == 'falcon':\n",
        "        return 'tiiuae/falcon-7b' if from_remote else 'base_models/falcon-7b'\n",
        "    elif name == 'internlm':\n",
        "        return 'internlm/internlm-7b' if from_remote else 'base_models/internlm-7b'\n",
        "    elif name == 'qwen':\n",
        "        return 'Qwen/Qwen-7B' if from_remote else 'base_models/Qwen-7B'\n",
        "    elif name == 'mpt':\n",
        "        return 'cekal/mpt-7b-peft-compatible' if from_remote else 'base_models/mpt-7b-peft-compatible'\n",
        "        # return 'mosaicml/mpt-7b' if from_remote else 'base_models/mpt-7b'\n",
        "    elif name == 'bloom':\n",
        "        return 'bigscience/bloom-7b1' if from_remote else 'base_models/bloom-7b1'\n",
        "    else:\n",
        "        raise ValueError(f\"Undefined base model {name}\")\n",
        "\n",
        "\n",
        "def load_dataset(names, from_remote=False):\n",
        "    dataset_names = [d for d in names.split(',')]\n",
        "    dataset_list = []\n",
        "    for name in dataset_names:\n",
        "        rep = 1\n",
        "        if not os.path.exists(name):\n",
        "            rep = int(name.split('*')[1]) if '*' in name else 1\n",
        "            name = ('FinGPT/fingpt-' if from_remote else 'data/fingpt-') + name.split('*')[0]\n",
        "        tmp_dataset = datasets.load_from_disk(name)\n",
        "        if 'test' not in tmp_dataset:\n",
        "            tmp_dataset = tmp_dataset.train_test_split(0.2, shuffle=True, seed=42)\n",
        "\n",
        "        dataset_list.extend([tmp_dataset] * rep)\n",
        "    return dataset_list\n",
        "\n"
      ],
      "metadata": {
        "id": "Wpf57NyVbuVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_tasks = [\n",
        "    'Financial Sentiment Analysis',\n",
        "    'Financial Relation Extraction',\n",
        "    'Financial Headline Classification',\n",
        "    'Financial Named Entity Recognition',\n",
        "]\n",
        "demo_inputs = [\n",
        "    \"Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano\",\n",
        "    \"Apple Inc Chief Executive Steve Jobs sought to soothe investor concerns about his health on Monday, saying his weight loss was caused by a hormone imbalance that is relatively simple to treat.\",\n",
        "    'gold trades in red in early trade; eyes near-term range at rs 28,300-28,600',\n",
        "    'This LOAN AND SECURITY AGREEMENT dated January 27 , 1999 , between SILICON VALLEY BANK (\" Bank \"), a California - chartered bank with its principal place of business at 3003 Tasman Drive , Santa Clara , California 95054 with a loan production office located at 40 William St ., Ste .',\n",
        "]\n",
        "demo_instructions = [\n",
        "    'What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}.',\n",
        "    'Given phrases that describe the relationship between two words/phrases as options, extract the word/phrase pair and the corresponding lexical relationship between them from the input text. The output format should be \"relation1: word1, word2; relation2: word3, word4\". Options: product/material produced, manufacturer, distributed by, industry, position held, original broadcaster, owned by, founded by, distribution format, headquarters location, stock exchange, currency, parent organization, chief executive officer, director/manager, owner of, operator, member of, employer, chairperson, platform, subsidiary, legal form, publisher, developer, brand, business division, location of formation, creator.',\n",
        "    'Does the news headline talk about price going up? Please choose an answer from {Yes/No}.',\n",
        "    'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',\n",
        "]"
      ],
      "metadata": {
        "id": "kTsoodGCHwDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(base_model, peft_model, from_remote=True):\n",
        "\n",
        "    model_name = parse_model_name(base_model, from_remote)\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name, trust_remote_code=True,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    model.model_parallel = True\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "    tokenizer.padding_side = \"left\"\n",
        "    if base_model == 'qwen':\n",
        "        tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids('<|endoftext|>')\n",
        "        tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('<|extra_0|>')\n",
        "    if not tokenizer.pad_token or tokenizer.pad_token_id == tokenizer.eos_token_id:\n",
        "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    model = PeftModel.from_pretrained(model, peft_model)\n",
        "    model = model.eval()\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def test_demo(model, tokenizer):\n",
        "\n",
        "    for task_name, input, instruction in zip(demo_tasks, demo_inputs, demo_instructions):\n",
        "        prompt = 'Instruction: {instruction}\\nInput: {input}\\nAnswer: '.format(\n",
        "            input=input,\n",
        "            instruction=instruction\n",
        "        )\n",
        "        inputs = tokenizer(\n",
        "            prompt, return_tensors='pt',\n",
        "            padding=True, max_length=512,\n",
        "            return_token_type_ids=False\n",
        "        )\n",
        "        inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
        "        res = model.generate(\n",
        "            **inputs,\n",
        "            max_length=512,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            use_cache=False\n",
        "        )\n",
        "        output = tokenizer.decode(res[0], skip_special_tokens=True)\n",
        "        print(f\"\\n==== {task_name} ====\\n\")\n",
        "        print(output)"
      ],
      "metadata": {
        "id": "yGj4oCaDZkjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FROM_REMOTE=True\n",
        "\n",
        "base_model = 'falcon'\n",
        "peft_model = 'FinGPT/fingpt-mt_falcon-7b_lora' if FROM_REMOTE else 'finetuned_models/MT-falcon-linear_202309210126'\n",
        "\n",
        "model, tokenizer = load_model(base_model, peft_model, FROM_REMOTE)"
      ],
      "metadata": {
        "id": "24-Nh82zaWuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_demo(model, tokenizer)"
      ],
      "metadata": {
        "id": "Ckn6fnNFL7Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hsO0Wg9y0h_E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}